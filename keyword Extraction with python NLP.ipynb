{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('papers.csv')\n",
    "df = df.iloc[:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ade16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa4fcb",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "# Working With \"paper text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033de3a",
   "metadata": {},
   "source": [
    "# Steps to do\n",
    "\n",
    "1 Lower case\n",
    "\n",
    "2 remove HTML tags\n",
    "\n",
    "3 remove special characters and digits\n",
    "\n",
    "4 Convert to list from string\n",
    "\n",
    "5 remove stopwords\n",
    "\n",
    "6 remove words less than three letters\n",
    "\n",
    "7 lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1172385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da7873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac463132",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"also\", \"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stop_words.union(new_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecca94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02be39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(txt):\n",
    "    txt = txt.lower()\n",
    "#     .)r'<.*>' is a regular expression pattern. In this case, it matches any \n",
    "# substring that starts with <, followed by any character (.* matches any character\n",
    "# any number of times), and ends with >. This pattern is used to match HTML\n",
    "# or XML-like tags (e.g., <tag> or <div>) because .* is a greedy match and \n",
    "# will match everything between < and >.\n",
    "    txt = re.sub(r'<.*>',' ',txt)\n",
    "    txt = re.sub(r'[^a-zA-Z]',' ',txt)\n",
    "    \n",
    "    #Tokenization\n",
    "    txt = nltk.word_tokenize(txt)\n",
    "    \n",
    "    #Removing stopwords \n",
    "    txt = [word for word in txt if word not in stop_words]\n",
    "    \n",
    "#     .0Accepting words that charcter are greater then 3.\n",
    "    txt = [word for word in txt if len(word) >3]\n",
    "    \n",
    "#     stemming\n",
    "    stemming = PorterStemmer()\n",
    "    txt = [stemming.stem(word) for word in txt]\n",
    "\n",
    "    return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text('This is python <hcsjngb> %^&*$ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['paper_text'].apply(lambda x:preprocessing_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb724e7c",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "Explanation of the parameters used in CountVectorizer:\n",
    "\n",
    "max_df: This parameter represents the maximum document frequency.\n",
    "It ignores terms that have a document frequency strictly higher\n",
    "than the given threshold (here, 0.95 means terms appearing in \n",
    "more than 95% of the documents will be ignored).\n",
    "\n",
    "max_features: It indicates the maximum number of features\n",
    "(or words/vocabulary) to be extracted. In this case, it's set to 5000,\n",
    "meaning only the top 5000 most frequent words will be used as features.\n",
    "\n",
    "ngram_range: This parameter specifies the lower and upper boundary \n",
    "of the range for n-grams to be extracted. In the corrected code,\n",
    "ngram_range=(1, 3) means it will extract unigrams, bigrams,\n",
    "and trigrams from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae462a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "cv = CountVectorizer(max_df=0.95,max_features=5000,ngram_range=(1, 2))\n",
    "word_count_vectors = cv.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45f1b7",
   "metadata": {},
   "source": [
    "# Using TF-IDF\n",
    "\n",
    ".)TF-IDF stands for Text Frequency Inverse Document Frequency.\n",
    "\n",
    "The importance of each word increases in proportion to the number \n",
    "\n",
    "of times a word appears in the document (Text Frequency – TF) but \n",
    "\n",
    "is offset by the frequency of the word in the corpus (Inverse Document Frequency – IDF).\n",
    "\n",
    "./)Using the tf-idf weighting scheme, the keywords are\n",
    "\n",
    "the words with the highest TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f169648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# .)smooth_idf=True: This parameter, when set to True, adds 1\n",
    "#     to the document frequencies (IDF smoothing) to prevent zero divisions.\n",
    "\n",
    "# .)use_idf=True: This parameter, when set to True, enables IDF and it wil give the high\n",
    "#  value to that worwds whose freq is less.\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2f2fd",
   "metadata": {},
   "source": [
    "# EXtracting Keywords\n",
    "Getting Features Names \n",
    "\n",
    "Word Counts for user Docs \n",
    "\n",
    "Sorting Sparse matrix cordinated \n",
    "\n",
    "Extracting top 10 keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Functions:\n",
    "sort_coo(coo_matrix):\n",
    "\n",
    "This function takes a sparse COO (Coordinate List) matrix (coo_matrix) as input.\n",
    "It uses zip() to pair the column indices (coo_matrix.col) with their corresponding \n",
    "data values (coo_matrix.data).\n",
    "It then sorts these pairs based on the values (x[1]) in descending order (reverse=True)\n",
    "and returns the sorted list of tuples.\n",
    "extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "\n",
    "    \n",
    "This function extracts the top 'n' items from a sorted list of tuples (sorted_items) \n",
    "containing index-value pairs.\n",
    "It rounds the scores to three decimal places and creates two lists: score_vals\n",
    "    for scores and feature_vals for feature names.\n",
    "It generates a dictionary results mapping feature names to their corresponding \n",
    "scores, limiting the entries to the top 'n' items.\n",
    "\n",
    "\n",
    "Main Code:\n",
    "Fetching Feature Names: feature_names = cv.get_feature_names_out() retrieves the\n",
    "    feature names (vocabulary) from a CountVectorizer (cv).\n",
    "\n",
    "    \n",
    "get_keywords(idx, docs):\n",
    "\n",
    "This function:\n",
    "Generates the TF-IDF vector for the document at index idx from the provided list of documents (docs).\n",
    "Sorts the TF-IDF vectors by descending order of scores.\n",
    "Extracts the top 10 keywords for the given document using the previously defined functions.\n",
    "print_results(idx, keywords, df):\n",
    "\n",
    "This function prints the title, abstract, and extracted keywords for a specified index (idx) in the provided DataFrame (df).\n",
    "Execution:\n",
    "\n",
    "idx = 941: Sets the index for which keywords are to be extracted.\n",
    "keywords = get_keywords(idx, docs): Retrieves the top keywords for the document \n",
    "    at index 941.\n",
    "print_results(idx, keywords, df): Prints the title, abstract, and extracted keywords\n",
    "    for the document at index 941.\n",
    "This code overall defines functions to extract keywords from text documents using\n",
    "TF-IDF representation and then prints out these extracted keywords along with \n",
    "information about a specific document from the provided dataset. Adjust the idx\n",
    "variable to test it with different documents in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9019238",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "#     .)The blw coo_matrix.col means the cols which are 5000 of vocablary and \n",
    "#     coo_matrix.data is the tfidf values for the words which appers less time it \n",
    "#     will make it interger value high.\n",
    "# .)The blw wuill zipd the dic each word with its value of tfidf.\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "#     .0And blw the tuple of zip will be sorted acording to the int valyue of tfidf in \n",
    "#     descending order so the big hih score velue will come first.\n",
    "    return sorted(tuples, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    #taking top items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        score_vals.append(round(score,3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    \n",
    "    #create a tuples of features,score\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]  # Fix: Changed '==' to '='\n",
    "    return results\n",
    "\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names_out()\n",
    "\n",
    "def get_keywords(idx, doc):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "#     .)The blw tocool will convert the vector into 2 cordinated .\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "idx=995\n",
    "keywords=get_keywords(idx, doc)\n",
    "print_results(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384d132",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------just for info-----------\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc[1]]))\n",
    "docs_words_count = tf_idf_vector.tocoo()\n",
    "tuples = zip(docs_words_count.col, docs_words_count.data)\n",
    "c = list(tuples)\n",
    "c\n",
    "# sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df06d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------just for info-----------\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc[1]]))\n",
    "docs_words_count = tf_idf_vector.tocoo()\n",
    "tuples = zip(docs_words_count.col, docs_words_count.data)\n",
    "sorted_items = sorted(tuples, key=lambda x: (x[1]), reverse=True)\n",
    "sorted_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3af5c8",
   "metadata": {},
   "source": [
    "# Using pickle to save model and vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump('cv',open('count_vector.pkl','wb'))\n",
    "pickle.dump('tfidf_transformer',open('tfidf_transformer.pkl','wb'))\n",
    "pickle.dump('feature_names',open('feature_names.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7efd1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415145ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ed871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd803d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666e455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caa012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122ad90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7d41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc9513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f912b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10ccd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be5018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
